###csi driver setup 2.11.0 ### democenter lab
-https://dell.github.io/csm-docs/docs/csidriver/release/powerscale/
-continue to use docker on democenter hosts based on CentOS version
-https://dell.github.io/csm-docs/docs/
-https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/

CSM & CSI Installation without OLM
1.install Snapshot CRD
2.install CSM Operator without OLM
3.create secrets
4.install CSI Operator


##https://orcacore.com/upgrade-rockylinux9-kernel-latest-version/

### get root
sudo su -

apt update
apt upgrade




#ensure dns is setup for all hosts, do not skip this step
#Add IP - hostnames of k8 hosts, fqdn and shortname

echo 192.168.1.40    u01.demo.local  u01 >> /etc/hosts
echo 192.168.1.41    u02demo.local   u02 >> /etc/hosts
echo 192.168.1.42    u03.demo.local  u03 >> /etc/hosts

hostnamectl set-hostname u01.demo.local
hostnamectl set-hostname u02.demo.localcat 
hostnamectl set-hostname u03.demo.local
hostnamectl


###################################################################################################################################################
###isilon setup 
###create the csi onefs api user

isi auth groups create --zone=System --name=k8 --provider=local
isi auth users create --zone=System --name=k8 --set-password --enabled=true --provider=local --primary-group=k8

(use Password123!)

isi auth roles create --name=k8-csi --description="K8-CSI role" --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_PAPI --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NFS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_QUOTA --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SNAPSHOT --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_IFS_RESTORE --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NS_IFS_ACCESS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SYNCIQ --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SYNCIQ_POLICIES --zone=System
isi auth roles modify k8-csi --add-user=k8 --zone=System
isi auth roles view k8-csi --zone=System
isi auth users view --zone=System --user=k8
mkdir /ifs/csi
chmod 777 /ifs/csi

isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_SSH --zone=System

### review api auth on onefs - session or basic
isi_gconfig -t web-config
isi_gconfig -t web-config auth_basic=true
isi_gconfig -t web-config auth_basic=false
https://x.x.x.x:8080/platform   -- can be used to test API access if basic = true


###################################################################################################################################################
###################################################################################################################################################

dnf makecache --refresh
dnf update -y

dnf install -y  device-mapper-persistent-data lvm2 nfs-utils git wget tar
systemctl start rpcbind
systemctl enable rpcbind
systemctl status rpcbind


setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config

systemctl stop firewalld
systemctl disable firewalld
systemctl status firewalld


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system


sed -i '/swap/d' /etc/fstab
swapoff -a
modprobe ip_tables
reboot


###check status
lsmod | grep br_netfilter
lsmod | grep overlay
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward



###################################################################################################################################################
###install containerd Runtime
### https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-rocky-linux-8


dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
dnf makecache
dnf install  --allowerasing containerd.io

mv /etc/containerd/config.toml /etc/containerd/config.toml.orig
containerd config default > /etc/containerd/config.toml

vi /etc/containerd/config.toml

###edit SystemdCgroup = true 


###need to add docker hub logon info to allow imahe pulls after exceeding rate limits
[plugins]

[plugins."io.containerd.grpc.v1.cri".registry.configs."registry-1.docker.io".auth]
    username = "<username>"
    password = "<password>"


systemctl enable --now containerd.service
systemctl status containerd.service


###################################################################################################################################################
###install Kubernetes
###https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

###install K8
###https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/
enabled=1
gpgcheck=1
gpgkey=https://pkgs.k8s.io/core:/stable:/v1.29/rpm/repodata/repomd.xml.key
EOF


###
Note:
To upgrade kubectl to another minor release, you'll need to bump the version in /etc/yum.repos.d/kubernetes.repo before running yum update. 
This procedure is described in more detail in Changing The Kubernetes Package Repository.

###check supported versions of k8s with CSI
cat /etc/yum.repos.d/kubernetes.repo
dnf list kube* --showduplicates --disableexcludes=kubernetes | sort -r

#install a supported version, see CSI powerscale docs
#dnf install kubelet kubeadm kubectl --disableexcludes=kubernetes


###with K8 1.29 - see supported versions
dnf makecache
dnf install kubelet-1.29.* kubeadm-1.29.* kubectl-1.29.* --disableexcludes=kubernetes

systemctl enable --now kubelet.service


###Complete up to here for all nodes going into cluster!!!!


########################################################################################################
###deploy kubernetes cluster on master node only, use output to join other nodes later
########################################################################################################
#kubeadm init --pod-network-cidr=X.X.X.X/X

kubeadm init --pod-network-cidr=10.244.0.0/16


###
<see output for node info and joining nodes to cluster - DO NOT JOIN YET> copy the node join code for later
###


mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
cat $HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf 
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /root/.bash_profile
cat /root/.bash_profile


###deploy a CNI pod network to the cluster.
###https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
### Install pod network plugin

##### Install CLI 

    CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
    CLI_ARCH=amd64
    if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
    curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
    sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
    tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
    rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
    
##### Install the plugin 
    cilium install 
[Link to the Cilium installation guide](https://docs.cilium.io/en/latest/gettingstarted/k8s-install-default/)    

##### check cilium status
    cilium status
    kubectl -n kube-system exec cilium-2hq5z -- cilium-dbg status
    cilium connectivity test



kubectl get pods --all-namespaces
kubectl cluster-info
kubectl cluster-info dump


###if only deploying a single node cluster - cluster will not schedule Pods on the control plane node - remove this taint
kubectl describe node | grep -i taint
### taints:             node-role.kubernetes.io/master:NoSchedule

kubectl taint nodes --all node-role.kubernetes.io/master-
kubectl describe node | grep -i taint


########################################################################################################
###build and setup containerd & K8 then join all other nodes to cluster here per the kubeadm init output###
########################################################################################################

###to obtain string
kubeadm token create --print-join-command


##eg:
kubeadm join 192.168.1.40:6443 --token 2bxzot.jiamo9pjlikgwmty \
    --discovery-token-ca-cert-hash sha256:7fdc56687d8b1ab6a04eddebd4e741ee7ba4f1b751179fee48b83c6449d5d08a 

###on master node
kubectl get nodes
kubectl get pods --all-namespaces



######################################################
###if reset of k8 cluster needed, run on master node
kubeadm reset
######################################################


# setup autocomplete in bash into the current shell, bash-completion package should be installed first. on Master node
source <(kubectl completion bash) 

# add autocomplete permanently to your bash shell.
echo "source <(kubectl completion bash)" >> ~/.bashrc 







###################################################################################################################################################
###csm & csi install
###################################################################################################################################################

###CSM & CSI Operators and driver install
### On K8 linux:
####https://dell.github.io/csm-docs/docs/deployment/csmoperator/
####https://dell.github.io/csm-docs/docs/deployment/csmoperator/drivers/


###################################################################################################################################################
###install helm3 on master
cd ~
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh


###################################################################################################################################################
###Installing CSI Driver for PowerScale via Dell CSM Operator
https://dell.github.io/csm-docs/docs/deployment/csmoperator/drivers/powerscale/
https://dell.github.io/csm-docs/docs/deployment/csmoperator/#manual-installation-on-a-cluster-without-olm
###mkdir for isilon csi configuration files
###create a link, makes upgrade easier

mkdir git
cd ~/git
git clone -b v2.11.0 https://github.com/dell/csi-powerscale.git
mv csi-powerscale csi-powerscale-2-11.0
ln -s /root/git/csi-powerscale-2-11.0 csi-latest
touch csi-latest/2-11.0.txt

###################################################################################################################################################
###add the snapshot CRD required by csm operator
cd ~/git
git clone https://github.com/kubernetes-csi/external-snapshotter/
cd ./external-snapshotter
git checkout v8.0.1
rm -f client/config/crd/kustomization.yaml
rm -f deploy/kubernetes/snapshot-controller/kustomization.yaml
kubectl create -f client/config/crd
kubectl create -f deploy/kubernetes/snapshot-controller
kubectl get pods -A
kubectl get crd



###################################################################################################################################################
###CSM Operator Installation on cluster without OLM
###https://dell.github.io/csm-docs/docs/deployment/csmoperator/#installation
###Clone and checkout the required csm-operator version using 
cd ~/git
git clone -b v1.6.0 https://github.com/dell/csm-operator.git
cd ~/git/csm-operator
bash scripts/install.sh

kubectl get pods -n dell-csm-operator

###if needed
##bash scripts/uninstall.sh
###

###################################################################################################################################################

###################################################################################################################################################
##Installing CSI Driver for PowerScale via Dell CSM Operator
https://dell.github.io/csm-docs/docs/deployment/csmoperator/drivers/powerscale/
###setup for the csi driver
###https://dell.github.io/csm-docs/docs/deployment/csmoperator/drivers/powerscale/
###create a K8 namespace for csi driver 

kubectl create namespace isilon
kubectl get ns



############################################################################################
### review api auth on onefs - session or basic
### 2.8 fixed session based auth with 9.5.4.0
isi_gconfig -t web-config
#isi_gconfig -t web-config auth_basic=true
#isi_gconfig -t web-config auth_basic=false
#set basic or session in my-settings.yaml
#session auth = 1 = true
#basic auth  = 0 = false
#https://x.x.x.x:8080/platform   -- can be used to test API access if basic = true
############################################################################################


mkdir ~/git/csi-latest/isilon
cd ~/git/csi-latest/isilon
curl -fsSL -o secret-2-11.0.yaml https://raw.githubusercontent.com/dell/csi-powerscale/v2.11.0/samples/secret/secret.yaml
curl -fsSL -o empty-secret-2-11.0.yaml  https://raw.github.com/dell/csi-powerscale/v2.11.0/samples/secret/empty-secret.yaml
vi secret-2-11.0.yaml


###modify
  
      - clusterName: "ps1"
      
      username: "k8"

      password: "Password123!"

      endpoint: "https://csi.demo.local"       < --- This is the System Zone IP or FQDN, for the driver to make API calls to

      isDefault: true

 ###delete/comment out the cluster2 controllerCount


###create isilon-creds secret 

kubectl create secret generic isilon-creds -n isilon --from-file=config=/root/git/csi-latest/isilon/secret-2-11.0.yaml -o yaml --dry-run=client | kubectl apply -f -


###since we are using isiInsecure = 'true"  in my-isilon-setting.yaml, need an empty secret


kubectl create -f ~/git/csi-latest/isilon/empty-secret-2-11.0.yaml
kubectl get secret -n isilon
kubectl get secret isilon-creds -o yaml -n isilon




###https://dell.github.io/csm-docs/docs/deployment/csmoperator/
###https://dell.github.io/csm-docs/docs/deployment/csmoperator/drivers/powerscale/
###Create a CR (Custom Resource) for PowerScale using the sample files:https://github.com/dell/dell-csi-operator/tree/master/samples

###query csi drivers 
kubectl get csm --all-namespaces

cd ~/git/csi-latest/isilon

#curl -fsSL -o my-isilon-settings-2-11.0.yaml https://github.com/dell/csm-operator/blob/main/samples/storage_csm_powerscale_v2110.yaml
#or copy from the csm 

cp  ~/git/csm-operator/samples/storage_csm_powerscale_v2110.yaml my-isilon-settings-2-11.0.yaml

vi my-isilon-settings-2-11.0.yaml

###edit my-isilon-settings-2-x.yaml: modify any values as needed
###replicas: 2    - if only 1 host, 2 is 2


apiVersion: storage.dell.com/v1
kind: ContainerStorageModule
 name: isilon
  namespace: isilon


  # Config version for CSI PowerScale v2.11.0 driver
    configVersion: v2.11.0
    authSecret: isilon-creds
    replicas: 2
    dnsPolicy: ClusterFirstWithHostNet


- name: X_CSI_ALLOWED_NETWORKS
          value: "[192.168.1.0/24], [172.17.0.0/18]"
  
- name: X_CSI_ISI_AUTH_TYPE
          value: "1"

 - name: X_CSI_ISI_PATH
          value: "/ifs/csi"
       
["--volume-name-prefix=k8s"]   'this is consistent, with the old csi drivers', optional


###enable resiliency 
- name: resiliency
      # enabled: Enable/Disable Resiliency feature
      # Allowed values:
      #   true: enable Resiliency feature(deploy podmon sidecar)
      #   false: disable Resiliency feature(do not deploy podmon sidecar)
      # Default value: false
      enabled: true
      configVersion: v1.9.0


###eanble replication if using



###install the csi driver operator
###add another session, to watch pod deployments during install or use k9s
watch kubectl get pods -A  



###install csi driver via csm 

kubectl create -f my-isilon-settings-2-11.0.yaml

###Review installation


kubectl get csm/isilon -n isilon -o yaml
kubectl describe csm isilon -n isilon

###edit the csi driver
kubectl edit csm/isilon -n isilon


###
###uninstall csi driver with CSM
kubectl delete csm/isilon -n isilon         --- uninstall CSI driver
cd ~git/csm-operator
bash scripts/uninstall.sh                   --- uninstall CSM Operator

kubectl get csm/isilon -n isilon -o yaml
kubectl get csm -A


###logs
kubectl logs <csm-operator-controller-podname> -n <namespace>
###

###################################################################################################################################################
###if you hit rate limit issue - logon to dockerhub if you have not
docker login
username
password


###review the csi pods
kubectl get pods -A
kubectl get pods --namespace isilon
kubectl describe pod   <pod>  -n isilon
kubectl logs  <pod> -n isilon -c driver


###K9S is very useful
mkdir ~/git/k9s
cd ~/git/k9s
wget https://github.com/derailed/k9s/releases/download/v0.25.12/k9s_Linux_x86_64.tar.gz
gunzip k9s_Linux_x86_64.tar.gz
tar -xvf k9s_Linux_x86_64.tar
ls -al
./k9s

https://k9scli.io/topics/commands/


###if needed b/c of issues starting the pods
-review the secrets.yaml
-review the my-settings.yaml
###update secrets
kubectl create secret generic isilon-creds -n isilon --from-file=config=/root/git/csi-isilon/isilon/secret-2-11.0.yaml -o yaml --dry-run=client | kubectl apply -f -







###################################################################################################################################################
USING the CSI Driver
###################################################################################################################################################
###Steps to create storage class: 
###There are samples storage class yaml files available under helm/samples/storageclass
###Sample storage classes:  samples/storageclass

cp ~/git/csi-latest/samples/storageclass/isilon.yaml  ~/git/csi-latest/isilon/my-isilon-sc-2-11.0.yaml
vi ~/git/csi-latest/isilon/my-isilon-sc-2-11.0.yaml

###modify the following as needed, otherwise csi & sc will use defaults from my-isilon-settings.yaml
###defines where the exports/dir/quota of the volumeis created; defaults to System Zone
### default sc name is isilon, can use or add version if needed; will need to modify samples and pvc files to support sc name
 
 AccessZone: System         < -- AZ you create exports in


 IsiPath /ifs/csi           < -- Path to directories in the AZ you create exports in


 AzServiceIP : csi.demo.local   < -- SCName or IP of the AZ you create nfs exports and access them, not System if using a different zone


###add mount options if needed
mountOptions: [vers=3]


###Other options can be reviewed and tested as needed

###copy and create a sample storageclass - sc-2-11.0  for driver version
kubectl create -f ~/git/csi-latest/isilon/my-isilon-sc-2-11.0.yaml
kubectl get sc
kubectl describe sc isilon


###################################################################################################################################################
###Testing CSI with samples
###sample test files:  ~/git/csi-latest/samples
###create a testing namespace

kubectl create namespace test
cd ~/git/csi-latest/samples

###create a pvc -- view and edit as needed
cat persistentvolumeclaim/pvc.yaml   

kubectl create -f persistentvolumeclaim/pvc.yaml -n test
kubectl get pvc -n test
kubectl describe pvc test-pvc -n test
kubectl get pv -n test
kubectl describe pv <pv name> -n test

###create sample pod and attach to pvc
kubectl create -f pod/nginx.yaml -n test
kubectl get pods -n test
kubectl describe pods nginx-pv-pod -n test

kubectl get pv -n test
kubectl exec nginx-pv-pod -i -t -n test -- bash 



###from isilon cli, view and test, the directory mounted by the pod

###go to isilon, review
###nfs 
isi nfs exports list --zone=System
###quota
isi quota list --zone=System
###add a file from cli

cd /ifs/csi/k8s-.../
touch this_is_isilon.txt
ls -al


###test csi snapshot snapshots
###VolumeSnapshotClass is needed for creating the volume snapshots

cd ~/git/csi-latest/samples
vi volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml

###modify path as needed

isiPath: "/ifs/csi"


###create the volumesnapshotclass in the namespace as the pvc and the pod

kubectl create -f volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml -n test
kubectl get Volumesnapshotclass -n test

###create the snapshot
kubectl get pvc -n test
vi volumesnapshot/snapshot-of-test-pvc.yaml

###review and set the namespace from default to: 
test

kubectl create -f volumesnapshot/snapshot-of-test-pvc.yaml -n test
kubectl get volumesnapshot -n test


###view snapshot from isilon
isi snapshot list
ls -al /ifs/.snapshot/snapshot-<id_created>/csi/<volume_name>


###view snapshot from nginxpod
kubectl exec ngnix-pv-pod -i -t -n test -- bash
cd /usr/share/nginx/html/.snapshot
cd <snapshot-id>
ls -al




###################################################################################################################################################
###cleanup the snap, pod and pvc
###delete the snapshot
kubectl get volumesnapshot -n test
kubectl delete volumesnapshot snapshot-of-test-pvc -n test
kubectl get volumesnapshot -n test

###cleanup; 
kubectl get pods -n test
kubectl delete -f pod/nginx.yaml -n test
kubectl get pods -n test

kubectl get pvc  -n test
kubectl delete pvc test-pvc -n test
kubectl get pvc  -n test
kubectl get pv


###on isilon
isi snapshot list
isi nfs exports list
isi quota list 


#pod, pvc and pv will be gone 
#export/quota and snap will be gone 
#have fun with CSI!! 


!completes setup and simple demo










###################################################################################################################################################
Misc stuff 


###useful
kubectl get pods -A
kubectl get pods --namespace isilon
kubectl describe pod     -n isilon
kubectl logs   -n isilon -c driver

cd ~/git/csi-isilon/dell-csi-helm-installer
./csi-uninstall.sh --namespace isilon 

cd ~/git/csi-isilon/dell-csi-helm-installer
./csi-install.sh --namespace isilon --values ~/git/csi-isilon/isilon/my-isilon-settings-2-x.yaml 

kubectl delete pv <k8s-ec5c9fec46>

kubectl get pv k8s-56da61f45b
kubectl describe pv k8s-56da61f45b
kubectl get volumeattachment -n test
kubectl delete volumeattachment csi-e38eb79827b44396bd1520ab916091671c4d696ea9bfd4d54be428aa80e4b586 -n test


#delete secrets
kubectl get secret -n isilon
kubectl delete secret isilon-certs-0 -n isilon
kubectl delete secret isilon-creds -n isilon


kubectl create secret generic isilon-creds -n isilon --from-file=config=/root/git/csi-isilon/isilon/secret-2-x.yaml -o yaml --dry-run=client | kubectl apply -f -
kubectl create -f ~/git/csi-isilon/isilon/empty-secret-2-x.yaml
kubectl get secret -n isilon
kubectl get secret isilon-creds -o yaml -n isilon