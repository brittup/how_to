###csi driver setup

https://dell.github.io/storage-plugin-docs/docs/installation/helm/isilon/#install-csi-driver-for-powerscale


#ensure dns is setup for all hosts
#Add IP - hostnames of k8 hosts
vi /etc/hosts
vi /etc/resolv.conf
yum install -y yum-utils yum-utils device-mapper-persistent-data lvm2 nfs-utils nfs-utils-lib git wget

systemctl start rpcbind
systemctl enable rpcbind
systemctl status rpcbind

###################################################################################################################################################
###install Docker Runtime, use a specific version
###From: https://docs.docker.com/engine/install/centos/
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh


systemctl enable docker.service
systemctl start docker
systemctl status docker
docker run -it --rm ubuntu /bin/bash
    exit
docker ps --all
docker images -a

###################################################################################################################################################
###install Kubernetes
###https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

service firewalld stop
systemctl disable firewalld
systemctl status firewalld

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sysctl --system
sed -i '/swap/d' /etc/fstab
swapoff -a
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config


reboot


sestatus

###install K8
cat <<EOF |tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

cat /etc/yum.repos.d/kubernetes.repo

###check supported versions of k8s with CSI
yum list kube* --showduplicates --disableexcludes=kubernetes | sort -r

#yum install kubelet-1.14.1* kubeadm-1.14.1* kubectl-1.14.1* --disableexcludes=kubernetes
#yum install kubelet-1.19.8-0 kubeadm-1.19.8-0 kubectl-1.19.8-0 --disableexcludes=kubernetes


yum install kubelet kubeadm kubectl --disableexcludes=kubernetes


systemctl enable kubelet

###deploy kubernetes cluster
kubeadm init --pod-network-cidr=10.244.0.0/16

<see output for node info>


mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
cat $HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf 
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /root/.bash_profile
cat /root/.bash_profile
docker ps --all


###
###deploy a pod network to the cluster.
###Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
###https://kubernetes.io/docs/concepts/cluster-administration/addons/
###https://www.weave.works/blog/weave-net-kubernetes-integration/
###kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


wget https://cloud.weave.works/k8s/v1.16/net.yaml --no-check-certificate
kubectl apply -f net.yaml



kubectl get pods --all-namespaces
kubectl get nodes
kubectl get pods -A


### if reset needed
kubeadm reset
###


###isilon setup 
isi auth groups create --zone=System --name=k8 --provider=local
isi auth users create --zone=System --name=k8 --set-password --enabled=true --provider=local --primary-group=k8

(use Password123!)

isi auth roles create --name=k8-csi --description="K8-CSI role" --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_PAPI --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NFS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_QUOTA --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SNAPSHOT --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_IFS_RESTORE --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NS_IFS_ACCESS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_SSH --zone=System
isi auth roles modify k8-csi --add-user=k8 --zone=System
isi auth roles view k8-csi --zone=System
isi auth users view --zone=System --user=k8
mkdir /ifs/csi
chmod 777 /ifs/csi


###################################################################################################################################################
###CSI driver install
### On K8 linux:
####https://github.com/dell/csi-powerscale
####https://dell.github.io/storage-plugin-docs/docs/installation/
####https://dell.github.io/storage-plugin-docs/docs/installation/helm/isilon/


source <(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first.
echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanently to your bash shell.


###install helm
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

###clone the powerscale csi repo
mkdir git
cd git
git clone https://github.com/dell/csi-isilon.git


###add the following
vi /etc/systemd/system/multi-user.target.wants/docker.service

[Service]

MountFlags=shared

systemctl daemon-reload
systemctl restart docker

####install the snapshot controller
####1.20, Snapshot controller versioned 4.0.0
###https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/deploy/kubernetes/snapshot-controller
###https://github.com/kubernetes-csi/external-snapshotter/tree/v4.0.0/client/config/crd


####Install Snapshot CRD
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml

###Install Snapshot Controller               
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml

kubectl get crd

cd ~/git/csi-isilon
cp helm/csi-isilon/values.yaml my-isilon-settings.yaml
vi my-isilon-settings.yaml

###edit my-isilon-settings.yaml: modify any values as needed

controllerCount: 1    - if only 1 host


kubectl create namespace isilon

####create secrets
cd csi-isilon/helm
vi secret.json

{
  "isilonClusters": [
    {
      "clusterName": "cluster1",
      "username": "k8",
      "password": "Password123!",
      "isiIP": "10.246.156.13",
      "isDefaultCluster": true
    },
  ]
}


###create isilon-creds secret
kubectl create secret generic isilon-creds -n isilon --from-file=config=secret.json

kubectl get secret isilon-creds -o yaml -n isilon

###since we are using isiInsecure = 'true"
kubectl create -f emptysecret.yaml

kubectl get secret -n isilon
kubectl get secret isilon-creds -o yaml -n isilon



####Before we install the driver, we will take the NoSchedule taint away from the master node
kubectl describe node <hostname> | grep Taint
    #taints:             node-role.kubernetes.io/master:NoSchedule
kubectl taint nodes <hostname> node-role.kubernetes.io/master-


cd csi-isilon/dell-csi-helm-installer

watch kubectl get pods -A  


 ./verify.sh --namespace isilon --values ~/git/csi-isilon/my-isilon-settings.yaml  

./csi-install.sh --namespace isilon --values ~/git/csi-isilon/my-isilon-settings.yaml 


####if you hit rate limit issue - logon to dockerhub
docker login
username
password


kubectl get pods -A
kubectl get pods --namespace isilon
kubectl describe pod   <pod>  -n isilon
kubectl logs  <pod> -n isilon -c driver



###if needed
 ./csi-uninstall.sh --namespace isilon


##################################
Steps to create storage class: 
There are samples storage class yaml files available under helm/samples/storageclass


####get the storage class
kubectl get sc








































###test driver, from csi-isilon
cd ~/git/csi-isilon/test/helm

kubectl create namespace test


### start the volumes
./starttest.sh -t 2vols -n test
kubectl get pv -A
kubectl get pvc -A
kubectl get pvc -n test
kubectl get pods -n test
kubectl describe pods -n test
kubectl exec -ti -n test isilontest-0 bash


### stop the volumes and cleanup
./stoptest.sh -t 2vols -n test


###################################################################################################################################################
Misc:


















