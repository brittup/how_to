
###Install for PowerScale CSI driver with Rocky Linux


###csi driver setup 2.5 ###
-https://dell.github.io/csm-docs/docs/csidriver/release/powerscale/
-https://dell.github.io/csm-docs/docs/
-https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/


#ensure dns is setup for all hosts, do not skip this step
#Add IP - hostnames of k8 hosts, fqdn and shortname

echo 10.231.153.183 csi01.hdfslab.com csi01 >> /etc/hosts
echo 10.231.153.184 csi02.hdfslab.com csi02>> /etc/hosts
echo 10.231.153.185 csi03.hdfslab.com csi03 >> /etc/hosts
cat /etc/hosts

hostnamectl set-hostname csi01.hdfslab.com
hostnamectl

dnf makecache --refresh
dnf update -y

dnf install -y  device-mapper-persistent-data lvm2 nfs-utils git wget
systemctl start rpcbind
systemctl enable rpcbind
systemctl status rpcbind


setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config

systemctl stop firewalld
systemctl disable firewalld
systemctl status firewalld


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

modprobe overlay
modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sysctl --system
sed -i '/swap/d' /etc/fstab
swapoff -a
reboot


###check status
lsmod | grep br_netfilter
lsmod | grep overlay
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward



###################################################################################################################################################
###install containerd Runtime

dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
dnf makecache
dnf install -y --allowerasing containerd.io

mv /etc/containerd/config.toml /etc/containerd/config.toml.orig
containerd config default > /etc/containerd/config.toml

vi /etc/containerd/config.toml

###edit SystemdCgroup = true 


###need to add docker hub logon info to allow imahe pulls after exceeding rate limits
[plugins]

[plugins."io.containerd.grpc.v1.cri".registry.configs."registry-1.docker.io".auth]
    username = "<username>"
    password = "<password>"


systemctl enable --now containerd.service
systemctl status containerd.service


###################################################################################################################################################
###install Kubernetes
###https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/

###install K8

cat <<EOF |tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF

cat /etc/yum.repos.d/kubernetes.repo

###check supported versions of k8s with CSI
dnf list kube* --showduplicates --disableexcludes=kubernetes | sort -r
#install a supported version, see CSI powerscale docs
#dnf install kubelet kubeadm kubectl --disableexcludes=kubernetes


###with K8 1.25 - see supported versions
dnf install kubelet-1.25.* kubeadm-1.25.* kubectl-1.25.* --disableexcludes=kubernetes

systemctl enable --now kubelet.service


###Complete up to here for all nodes going into cluster!!!!




###
###deploy kubernetes cluster on master node only, use output to join other nodes later
###
#kubeadm init --pod-network-cidr=X.X.X.X/X
kubeadm init --pod-network-cidr=10.244.0.0/16

###
<see output for node info and joining nodes to cluster - DO NOT JOIN YET>
copy the node join code for later
###


mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
cat $HOME/.kube/config
export KUBECONFIG=/etc/kubernetes/admin.conf 
echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> /root/.bash_profile
cat /root/.bash_profile


###deploy a pod network to the cluster.
###Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
###https://www.weave.works/docs/net/latest/kubernetes/kube-addon/

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

kubectl get pods --all-namespaces
kubectl cluster-info
kubectl cluster-info dump


###if only deploying a single node cluster - cluster will not schedule Pods on the control plane node - remove this taint
kubectl describe node | grep -i taint
### taints:             node-role.kubernetes.io/master:NoSchedule

kubectl taint nodes --all node-role.kubernetes.io/master-
kubectl describe node | grep -i taint


############
###build and setup containerd & K8 then join other nodes to cluster here per the kubeadm init output###
############

##eg:
kubeadm join 192.168.1.40:6443 --token 2bxzot.jiamo9pjlikgwmty \
    --discovery-token-ca-cert-hash sha256:7fdc56687d8b1ab6a04eddebd4e741ee7ba4f1b751179fee48b83c6449d5d08a 

###to obtain string
kubeadm token create --print-join-command

###on master node
kubectl get nodes
kubectl get pods --all-namespaces


### 
###if reset of k8 cluster needed, run on master node
kubeadm reset
###





###################################################################################################################################################
###isilon setup 
###hit q and yes to accept eula

isi license add --evaluation=ONEFS,SMARTCONNECT_ADVANCED,SMARTQUOTAS,SNAPSHOTIQ,SYNCIQ
isi license list

###setup SmartConnect on Isilon

isi network subnets modify groupnet0.subnet0 --sc-service-addr=192.168.1.100 --sc-service-name=ssip-isilon.demo.local
isi network pools create --id=groupnet0:subnet0:pool1 --ranges=192.168.1.60-192.168.1.80  --access-zone=System  --alloc-method=dynamic  --ifaces=1-4:ext-1 --sc-subnet=subnet0  --sc-dns-zone=csi.demo.local    --description=csi_pool
isi network pools list
isi network pools view groupnet0:subnet0:pool1

###Run the following PowerShell command:

-open PowerShell prompt from Start Menu
Add-DnsServerZoneDelegation -Name "demo.local" -ChildZoneName "csi" -NameServer "ssip-isilon.demo.local" -IPAddress 192.168.1.100

nslookup csi.demo.local

###create the csi onefs api user

isi auth groups create --zone=System --name=k8 --provider=local
isi auth users create --zone=System --name=k8 --set-password --enabled=true --provider=local --primary-group=k8

(use Password123!)

isi auth roles create --name=k8-csi --description="K8-CSI role" --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_PAPI --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NFS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_QUOTA --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SNAPSHOT --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_IFS_RESTORE --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_NS_IFS_ACCESS --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_LOGIN_SSH --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SYNCIQ --zone=System
isi auth roles modify k8-csi --add-priv=ISI_PRIV_SYNCIQ_POLICIES --zone=System
isi auth roles modify k8-csi --add-user=k8 --zone=System
isi auth roles view k8-csi --zone=System
isi auth users view --zone=System --user=k8
mkdir /ifs/csi
chmod 777 /ifs/csi

###################################################################################################################################################




###CSI driver install
### On K8 linux:
####https://github.com/dell/csi-powerscale
####https://dell.github.io/storage-plugin-docs/docs/installation/
####https://dell.github.io/storage-plugin-docs/docs/installation/helm/isilon/

# setup autocomplete in bash into the current shell, bash-completion package should be installed first. on Master node
source <(kubectl completion bash) 

# add autocomplete permanently to your bash shell.
echo "source <(kubectl completion bash)" >> ~/.bashrc 

###################################################################################################################################################
###install helm3 on master
cd ~
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
chmod 700 get_helm.sh
./get_helm.sh

###################################################################################################################################################
###clone the powerscale csi repo
mkdir ~/git
cd git
git clone -b v2.5.0 https://github.com/dell/csi-powerscale.git
mv csi-powerscale csi-powerscale-2.5

###create a link, makes upgrade easier
ln -s /root/git/csi-powerscale-2.5 csi-isilon

###mkdir for isilon csi configuration files
mkdir ~/git/csi-isilon/isilon/

###add release version to linked directory
cd ~/git/csi-isilon
cat ReleaseNotes.md


touch CSI_2.5.txt
ls -al


*************************************
###The mount propagation in Docker must be configured on all Kubernetes nodes before installing CSI Driver for PowerScale.
###add the following
vi /etc/systemd/system/multi-user.target.wants/docker.service
[Service]
MountFlags=shared
systemctl daemon-reload
systemctl restart docker
*************************************



####install the snapshot controller
####Snapshot controller versioned, look at latest or specific versions per the CSM doc

https://github.com/kubernetes-csi/external-snapshotter/blob/master/README.md#csi-snapshotter

###2.5  -- select version: 6.1.0
###manually
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.1.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.1.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.1.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
###Install Snapshot Controller               
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.1.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v6.1.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml

###OR - one shot install

cd ~/git/
git clone https://github.com/kubernetes-csi/external-snapshotter/
cd ./external-snapshotter
git checkout v6.1.0   
rm -f client/config/crd/kustomization.yaml
rm -f deploy/kubernetes/snapshot-controller/kustomization.yaml
ls -al

kubectl create -f client/config/crd
kubectl create -f deploy/kubernetes/snapshot-controller
kubectl get crd

###################################################################################################################################################
###install the csi driver

###create a K8 namespace 
kubectl create namespace isilon


###configure the csi values & secret files


####create secrets
cp ~/git/csi-isilon/samples/secret/secret.yaml  ~/git/csi-isilon/isilon/secret.yaml
vi secret.yaml

###modify
  
      - clusterName: "ps1"
      
      username: "k8"

      password: "Password123!"

      endpoint: "https://csi.demo.local"       < --- This is the System Zone SmartConnect Name or IP in System Zone, for the driver to make API cals to

      isDefault: true

 ###delete/comment out the cluster2 controllerCount


###create isilon-creds secret 
kubectl create secret generic isilon-creds -n isilon --from-file=config=/root/git/csi-isilon/isilon/secret.yaml -o yaml --dry-run=client | kubectl apply -f -

###since we are using isiInsecure = 'true"  in my-isilon-setting.yaml, need an empty secret
cp ~/git/csi-isilon/samples/secret/empty-secret.yaml  ~/git/csi-isilon/isilon/empty-secret.yaml
cat empty-secret.yaml

kubectl create -f /root/git/csi-isilon/isilon/empty-secret.yaml
kubectl get secret -n isilon
kubectl get secret isilon-creds -o yaml -n isilon



####create settings
cd ~/git/csi-isilon/isilon/
cp ~/git/csi-isilon/helm/csi-isilon/values.yaml ~/git/csi-isilon/isilon/my-isilon-settings.yaml

vi my-isilon-settings.yaml

###Review auth seetings on onefs
isi_gconfig -t web-config auth_basic

###edit my-isilon-settings.yaml: modify any values as needed

controllerCount: 2    - if only 1 host, 2 is 2

healthMonitor:
  healthMonitor:true

isiAuthType: 0

isiPath: /ifs/data

###replication if using replication with repctl

###################################################################################################################################################

####Before we install the driver, we will take the NoSchedule taint away from the master node, USE SHORTNAME
###not required with 3 node k8 clusters
kubectl describe node <hostname> | grep Taint
    #taints:             node-role.kubernetes.io/master:NoSchedule
kubectl taint nodes <shortname of host> node-role.kubernetes.io/master-
kubectl describe node <hostname> | grep -i taint


###################################################################################################################################################
###install the csi driver
###add another session, to watch pod deployments during install

watch kubectl get pods -A  

cd ~/git/csi-isilon/dell-csi-helm-installer

./csi-install.sh --namespace isilon --values ~/git/csi-isilon/isilon/my-isilon-settings.yaml 


###watch the pods in the other session

####if you hit rate limit issue 
###need to add docker hub logon info to allow imahe pulls after exceeding rate limits
[plugins]

[plugins."io.containerd.grpc.v1.cri".registry.configs."registry-1.docker.io".auth]
    username = "<username>"
    password = "<password>"

 restart containerd and kubectl   
####if you hit rate limit issue 


###review the csi pods
kubectl get pods -A
kubectl get pods --namespace isilon
kubectl describe pod   <pod>  -n isilon
kubectl logs  <pod> -n isilon -c driver


###K9S is very useful
mkdir ~/git/k9s
cd ~/git/k9s
wget https://github.com/derailed/k9s/releases/download/v0.25.12/k9s_Linux_x86_64.tar.gz
gunzip k9s_Linux_x86_64.tar.gz
tar -xvf k9s_Linux_x86_64.tar
ls -al
./k9s

https://k9scli.io/topics/commands/


###Troubleshooting - review logs and describe pods
###if needed b/c of issues starting the pods
-review the secrets.yaml
-review the my-settings.yaml

###if needed uninstall the driver
 ./csi-uninstall.sh --namespace isilon

###recreate modify secret.yaml file
kubectl get secret -n isilon
kubectl delete secret isilon-creds -n isilon
kubectl create secret generic isilon-creds -n isilon --from-file=config=/root/git/csi-isilon/isilon/secret.yaml -o yaml --dry-run=client | kubectl apply -f -


###################################################################################################################################################
###Steps to create storage class: 
###There are samples storage class yaml files available under helm/samples/storageclass

###Sample storage classes:  samples/storageclass

cp ~/git/csi-isilon/samples/storageclass/isilon.yaml  ~/git/csi-isilon/isilon/my-isilon-sc.yaml
vi ~/git/csi-isilon/isilon/my-isilon-sc.yaml

###modify the following as needed, otherwise csi & sc will use defaults from my-isilon-settings.yaml
###defines where the exports/dir/quota of the volumeis created; defaults to System Zone
 AccessZone: System         < -- AZ you create exports in


 IsiPath /ifs/csi           < -- Path to directories in the AZ you create exports in


 AzServiceIP : 192.168.1.1  < -- SCName or IP of the AZ you are creating nfs exports in


###add mount options if needed
mountOptions: []


###Other options can be reviewed and tested as needed



###copy and create a sample storageclass - sc
kubectl create -f ~/git/csi-isilon/isilon/my-isilon-sc.yaml
kubectl get sc 


###################################################################################################################################################
###Testing CSI with samples
###sample test files:  ~/git/csi-isilon/samples

###create a testing namespace in K8
kubectl create namespace test

cd ~/git/csi-isilon/samples


###create a pvc
cat persistentvolumeclaim/pvc.yaml
kubectl create -f persistentvolumeclaim/pvc.yaml -n test
kubectl get pvc -n test
kubectl describe pvc test-pvc -n test
kubectl get pv -n test
kubectl describe pv <pv name> -n test


###create sample pod and attach to pvc
kubectl create -f pod/nginx.yaml -n test
kubectl get pods -n test
kubectl describe pods ngnix-pv-pod -n test

kubectl get pv -n test
kubectl exec ngnix-pv-pod -i -t -n test -- bash 

cd /usr/share/nginx/html/
ls -al
touch test1.txt
ls -al

###from isilon cli, view and test, the directory mounted by the pod

###go to isilon, review
###nfs 
isi nfs exports list --zone=System
###quota
isi quota list --zone=System
###add a file from cli

cd /ifs/csi/k8s-.../
touch this_is_isilon.txt
ls -al


###test csi snapshot snapshots
###VolumeSnapshotClass is needed for creating the volume snapshots

cd ~/git/csi-isilon/samples
vi volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml

###modify path as needed

isiPath: "/ifs/csi"


###create the volumesnapshotclass in the namespace as the pvc and the pod

kubectl create -f volumesnapshotclass/isilon-volumesnapshotclass-v1.yaml -n test
kubectl get Volumesnapshotclass -n test

###create the snapshot
kubectl get pvc -n test
vi volumesnapshot/snapshot-of-test-pvc.yaml

###review and set the namespace from default to: 
test

kubectl create -f volumesnapshot/snapshot-of-test-pvc.yaml -n test
kubectl get volumesnapshot -n test


###view snapshot from isilon
isi snapshot list
ls -al /ifs/.snapshot/snapshot-<id_created>/csi/<volume_name>


###view snapshot from nginxpod
kubectl exec ngnix-pv-pod -i -t -n test -- bash
cd /usr/share/nginx/html/.snapshot
cd <snapshot-id>
ls -al




###################################################################################################################################################
###cleanup the snap, pod and pvc
###delete the snapshot
kubectl get volumesnapshot -n test
kubectl delete volumesnapshot snapshot-of-test-pvc -n test
kubectl get volumesnapshot -n test

###cleanup; 
kubectl get pods -n test
kubectl delete -f pod/nginx.yaml -n test
kubectl get pods -n test

kubectl get pvc  -n test
kubectl delete pvc test-pvc -n test
kubectl get pvc  -n test
kubectl get pv


###on isilon
isi snapshot list
isi nfs exports list
isi quota list 


#pod, pvc and pv will be gone 
#export/quota and snap will be gone 
#have fun with CSI!! 


!completes setup and simple demo










###################################################################################################################################################
Misc stuff 


###useful
kubectl get pods -A
kubectl get pods --namespace isilon
kubectl describe pod     -n isilon
kubectl logs   -n isilon -c driver

cd ~/git/csi-isilon/dell-csi-helm-installer

./csi-uninstall.sh --namespace isilon 

kubectl delete pv <k8s-ec5c9fec46>

kubectl get pv k8s-56da61f45b
kubectl describe pv k8s-56da61f45b
kubectl get volumeattachment -n test
kubectl delete volumeattachment csi-e38eb79827b44396bd1520ab916091671c4d696ea9bfd4d54be428aa80e4b586 -n test


#delete secrets
kubectl get secret -n isilon
kubectl delete secret isilon-certs-0 -n isilon
kubectl delete secret isilon-creds -n isilon
kubectl get secret -n isilon